{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Installing NLTK ** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing NLTK Library\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Stop Words\n",
    "#First use nltk.download('stopwords') to download stop words\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"Thank you all so very much. Thank you to the Academy. \n",
    "               Thank you to all of you in this room. I have to congratulate \n",
    "               the other incredible nominees this year. The Revenant was \n",
    "               the product of the tireless efforts of an unbelievable cast\n",
    "               and crew. First off, to my brother in this endeavor, Mr. Tom \n",
    "               Hardy. Tom, your talent on screen can only be surpassed by \n",
    "               your friendship off screen … thank you for creating a t\n",
    "               ranscendent cinematic experience. Thank you to everybody at \n",
    "               Fox and New Regency … my entire team. I have to thank \n",
    "               everyone from the very onset of my career … To my parents; \n",
    "               none of this would be possible without you. And to my \n",
    "               friends, I love you dearly; you know who you are. And lastly,\n",
    "               I just want to say this: Making The Revenant was about\n",
    "               man's relationship to the natural world. A world that we\n",
    "               collectively felt in 2015 as the hottest year in recorded\n",
    "               history. Our production needed to move to the southern\n",
    "               tip of this planet just to be able to find snow. Climate\n",
    "               change is real, it is happening right now. It is the most\n",
    "               urgent threat facing our entire species, and we need to work\n",
    "               collectively together and stop procrastinating. We need to\n",
    "               support leaders around the world who do not speak for the \n",
    "               big polluters, but who speak for all of humanity, for the\n",
    "               indigenous people of the world, for the billions and \n",
    "               billions of underprivileged people out there who would be\n",
    "               most affected by this. For our children’s children, and \n",
    "               for those people out there whose voices have been drowned\n",
    "               out by the politics of greed. I thank you all for this \n",
    "               amazing award tonight. Let us not take this planet for \n",
    "               granted. I do not take tonight for granted. Thank you so very much.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For getting all the sentences from the paragraph\n",
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get all the word from the paragraph\n",
    "#words = nltk.word_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a porter stemmer object\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    new_words = [stemmer.stem(word) for word in words]\n",
    "    #Joining all the new words to get back sentence list\n",
    "    sentences[i] = ' '.join(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thank you all so veri much .',\n",
       " 'thank you to the academi .',\n",
       " 'thank you to all of you in thi room .',\n",
       " 'I have to congratul the other incred nomine thi year .',\n",
       " 'the reven wa the product of the tireless effort of an unbeliev cast and crew .',\n",
       " 'first off , to my brother in thi endeavor , mr. tom hardi .',\n",
       " 'tom , your talent on screen can onli be surpass by your friendship off screen … thank you for creat a t ranscend cinemat experi .',\n",
       " 'thank you to everybodi at fox and new regenc … my entir team .',\n",
       " 'I have to thank everyon from the veri onset of my career … To my parent ; none of thi would be possibl without you .',\n",
       " 'and to my friend , I love you dearli ; you know who you are .',\n",
       " \"and lastli , I just want to say thi : make the reven wa about man 's relationship to the natur world .\",\n",
       " 'A world that we collect felt in 2015 as the hottest year in record histori .',\n",
       " 'our product need to move to the southern tip of thi planet just to be abl to find snow .',\n",
       " 'climat chang is real , it is happen right now .',\n",
       " 'It is the most urgent threat face our entir speci , and we need to work collect togeth and stop procrastin .',\n",
       " 'We need to support leader around the world who do not speak for the big pollut , but who speak for all of human , for the indigen peopl of the world , for the billion and billion of underprivileg peopl out there who would be most affect by thi .',\n",
       " 'for our children ’ s children , and for those peopl out there whose voic have been drown out by the polit of greed .',\n",
       " 'I thank you all for thi amaz award tonight .',\n",
       " 'let us not take thi planet for grant .',\n",
       " 'I do not take tonight for grant .',\n",
       " 'thank you so veri much .']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm_sentences = nltk.sent_tokenize(paragraph)\n",
    "#Creating a object of lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization\n",
    "for i in range(len(lemm_sentences)):\n",
    "    words = nltk.word_tokenize(lemm_sentences[i])\n",
    "    new_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    lemm_sentences[i] = ' '.join(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thank you all so very much .',\n",
       " 'Thank you to the Academy .',\n",
       " 'Thank you to all of you in this room .',\n",
       " 'I have to congratulate the other incredible nominee this year .',\n",
       " 'The Revenant wa the product of the tireless effort of an unbelievable cast and crew .',\n",
       " 'First off , to my brother in this endeavor , Mr. Tom Hardy .',\n",
       " 'Tom , your talent on screen can only be surpassed by your friendship off screen … thank you for creating a t ranscendent cinematic experience .',\n",
       " 'Thank you to everybody at Fox and New Regency … my entire team .',\n",
       " 'I have to thank everyone from the very onset of my career … To my parent ; none of this would be possible without you .',\n",
       " 'And to my friend , I love you dearly ; you know who you are .',\n",
       " \"And lastly , I just want to say this : Making The Revenant wa about man 's relationship to the natural world .\",\n",
       " 'A world that we collectively felt in 2015 a the hottest year in recorded history .',\n",
       " 'Our production needed to move to the southern tip of this planet just to be able to find snow .',\n",
       " 'Climate change is real , it is happening right now .',\n",
       " 'It is the most urgent threat facing our entire specie , and we need to work collectively together and stop procrastinating .',\n",
       " 'We need to support leader around the world who do not speak for the big polluter , but who speak for all of humanity , for the indigenous people of the world , for the billion and billion of underprivileged people out there who would be most affected by this .',\n",
       " 'For our child ’ s child , and for those people out there whose voice have been drowned out by the politics of greed .',\n",
       " 'I thank you all for this amazing award tonight .',\n",
       " 'Let u not take this planet for granted .',\n",
       " 'I do not take tonight for granted .',\n",
       " 'Thank you so very much .']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemm_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Removing Stop Words **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thank much .',\n",
       " 'Thank Academy .',\n",
       " 'Thank room .',\n",
       " 'I congratulate incredible nominees year .',\n",
       " 'The Revenant product tireless efforts unbelievable cast crew .',\n",
       " 'First , brother endeavor , Mr. Tom Hardy .',\n",
       " 'Tom , talent screen surpassed friendship screen … thank creating ranscendent cinematic experience .',\n",
       " 'Thank everybody Fox New Regency … entire team .',\n",
       " 'I thank everyone onset career … To parents ; none would possible without .',\n",
       " 'And friends , I love dearly ; know .',\n",
       " \"And lastly , I want say : Making The Revenant man 's relationship natural world .\",\n",
       " 'A world collectively felt 2015 hottest year recorded history .',\n",
       " 'Our production needed move southern tip planet able find snow .',\n",
       " 'Climate change real , happening right .',\n",
       " 'It urgent threat facing entire species , need work collectively together stop procrastinating .',\n",
       " 'We need support leaders around world speak big polluters , speak humanity , indigenous people world , billions billions underprivileged people would affected .',\n",
       " 'For children ’ children , people whose voices drowned politics greed .',\n",
       " 'I thank amazing award tonight .',\n",
       " 'Let us take planet granted .',\n",
       " 'I take tonight granted .',\n",
       " 'Thank much .']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logic to remove stop words from sentences\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    #Removing all the words which are present in stopword lib\n",
    "    new_words = [word for word in words if word not in stopwords.words('english')]\n",
    "    #Creating sentences on new words\n",
    "    sentences[i] = ' '.join(new_words)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parts of Speech Tagging\n",
    "pot_words = nltk.word_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tagging Parts of Speech to each words\n",
    "pot_words_tags = nltk.pos_tag(pot_words)\n",
    "#This returns a list of tuples [('Thank', 'NNP'),('you', 'PRP'),..]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Thank', 'NNP'),\n",
       " ('you', 'PRP'),\n",
       " ('all', 'DT'),\n",
       " ('so', 'RB'),\n",
       " ('very', 'RB'),\n",
       " ('much', 'JJ'),\n",
       " ('.', '.'),\n",
       " ('Thank', 'VB'),\n",
       " ('you', 'PRP'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('Academy', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('Thank', 'NNP'),\n",
       " ('you', 'PRP'),\n",
       " ('to', 'TO'),\n",
       " ('all', 'DT'),\n",
       " ('of', 'IN'),\n",
       " ('you', 'PRP'),\n",
       " ('in', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('room', 'NN'),\n",
       " ('.', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('have', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('congratulate', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('other', 'JJ'),\n",
       " ('incredible', 'JJ'),\n",
       " ('nominees', 'NNS'),\n",
       " ('this', 'DT'),\n",
       " ('year', 'NN'),\n",
       " ('.', '.'),\n",
       " ('The', 'DT'),\n",
       " ('Revenant', 'NNP'),\n",
       " ('was', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('product', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('tireless', 'NN'),\n",
       " ('efforts', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('an', 'DT'),\n",
       " ('unbelievable', 'JJ'),\n",
       " ('cast', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('crew', 'NN'),\n",
       " ('.', '.'),\n",
       " ('First', 'NNP'),\n",
       " ('off', 'RB'),\n",
       " (',', ','),\n",
       " ('to', 'TO'),\n",
       " ('my', 'PRP$'),\n",
       " ('brother', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('endeavor', 'NN'),\n",
       " (',', ','),\n",
       " ('Mr.', 'NNP'),\n",
       " ('Tom', 'NNP'),\n",
       " ('Hardy', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('Tom', 'NNP'),\n",
       " (',', ','),\n",
       " ('your', 'PRP$'),\n",
       " ('talent', 'NN'),\n",
       " ('on', 'IN'),\n",
       " ('screen', 'NN'),\n",
       " ('can', 'MD'),\n",
       " ('only', 'RB'),\n",
       " ('be', 'VB'),\n",
       " ('surpassed', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('your', 'PRP$'),\n",
       " ('friendship', 'NN'),\n",
       " ('off', 'IN'),\n",
       " ('screen', 'JJ'),\n",
       " ('…', 'NNP'),\n",
       " ('thank', 'NN'),\n",
       " ('you', 'PRP'),\n",
       " ('for', 'IN'),\n",
       " ('creating', 'VBG'),\n",
       " ('a', 'DT'),\n",
       " ('t', 'JJ'),\n",
       " ('ranscendent', 'NN'),\n",
       " ('cinematic', 'JJ'),\n",
       " ('experience', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Thank', 'NNP'),\n",
       " ('you', 'PRP'),\n",
       " ('to', 'TO'),\n",
       " ('everybody', 'VB'),\n",
       " ('at', 'IN'),\n",
       " ('Fox', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('New', 'NNP'),\n",
       " ('Regency', 'NNP'),\n",
       " ('…', 'NNP'),\n",
       " ('my', 'PRP$'),\n",
       " ('entire', 'JJ'),\n",
       " ('team', 'NN'),\n",
       " ('.', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('have', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('thank', 'VB'),\n",
       " ('everyone', 'NN'),\n",
       " ('from', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('very', 'RB'),\n",
       " ('onset', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('my', 'PRP$'),\n",
       " ('career', 'NN'),\n",
       " ('…', 'NN'),\n",
       " ('To', 'TO'),\n",
       " ('my', 'PRP$'),\n",
       " ('parents', 'NNS'),\n",
       " (';', ':'),\n",
       " ('none', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('would', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('possible', 'JJ'),\n",
       " ('without', 'IN'),\n",
       " ('you', 'PRP'),\n",
       " ('.', '.'),\n",
       " ('And', 'CC'),\n",
       " ('to', 'TO'),\n",
       " ('my', 'PRP$'),\n",
       " ('friends', 'NNS'),\n",
       " (',', ','),\n",
       " ('I', 'PRP'),\n",
       " ('love', 'VBP'),\n",
       " ('you', 'PRP'),\n",
       " ('dearly', 'RB'),\n",
       " (';', ':'),\n",
       " ('you', 'PRP'),\n",
       " ('know', 'VBP'),\n",
       " ('who', 'WP'),\n",
       " ('you', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('.', '.'),\n",
       " ('And', 'CC'),\n",
       " ('lastly', 'RB'),\n",
       " (',', ','),\n",
       " ('I', 'PRP'),\n",
       " ('just', 'RB'),\n",
       " ('want', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('say', 'VB'),\n",
       " ('this', 'DT'),\n",
       " (':', ':'),\n",
       " ('Making', 'VBG'),\n",
       " ('The', 'DT'),\n",
       " ('Revenant', 'NNP'),\n",
       " ('was', 'VBD'),\n",
       " ('about', 'IN'),\n",
       " ('man', 'NN'),\n",
       " (\"'s\", 'POS'),\n",
       " ('relationship', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('natural', 'JJ'),\n",
       " ('world', 'NN'),\n",
       " ('.', '.'),\n",
       " ('A', 'DT'),\n",
       " ('world', 'NN'),\n",
       " ('that', 'IN'),\n",
       " ('we', 'PRP'),\n",
       " ('collectively', 'RB'),\n",
       " ('felt', 'VBD'),\n",
       " ('in', 'IN'),\n",
       " ('2015', 'CD'),\n",
       " ('as', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('hottest', 'JJS'),\n",
       " ('year', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('recorded', 'JJ'),\n",
       " ('history', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Our', 'PRP$'),\n",
       " ('production', 'NN'),\n",
       " ('needed', 'VBN'),\n",
       " ('to', 'TO'),\n",
       " ('move', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('southern', 'JJ'),\n",
       " ('tip', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('planet', 'NN'),\n",
       " ('just', 'RB'),\n",
       " ('to', 'TO'),\n",
       " ('be', 'VB'),\n",
       " ('able', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('find', 'VB'),\n",
       " ('snow', 'JJ'),\n",
       " ('.', '.'),\n",
       " ('Climate', 'NNP'),\n",
       " ('change', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('real', 'JJ'),\n",
       " (',', ','),\n",
       " ('it', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('happening', 'VBG'),\n",
       " ('right', 'RB'),\n",
       " ('now', 'RB'),\n",
       " ('.', '.'),\n",
       " ('It', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('most', 'RBS'),\n",
       " ('urgent', 'JJ'),\n",
       " ('threat', 'NN'),\n",
       " ('facing', 'VBG'),\n",
       " ('our', 'PRP$'),\n",
       " ('entire', 'JJ'),\n",
       " ('species', 'NNS'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('we', 'PRP'),\n",
       " ('need', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('work', 'VB'),\n",
       " ('collectively', 'RB'),\n",
       " ('together', 'RB'),\n",
       " ('and', 'CC'),\n",
       " ('stop', 'VB'),\n",
       " ('procrastinating', 'NN'),\n",
       " ('.', '.'),\n",
       " ('We', 'PRP'),\n",
       " ('need', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('support', 'VB'),\n",
       " ('leaders', 'NNS'),\n",
       " ('around', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('world', 'NN'),\n",
       " ('who', 'WP'),\n",
       " ('do', 'VBP'),\n",
       " ('not', 'RB'),\n",
       " ('speak', 'VB'),\n",
       " ('for', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('big', 'JJ'),\n",
       " ('polluters', 'NNS'),\n",
       " (',', ','),\n",
       " ('but', 'CC'),\n",
       " ('who', 'WP'),\n",
       " ('speak', 'VBP'),\n",
       " ('for', 'IN'),\n",
       " ('all', 'DT'),\n",
       " ('of', 'IN'),\n",
       " ('humanity', 'NN'),\n",
       " (',', ','),\n",
       " ('for', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('indigenous', 'JJ'),\n",
       " ('people', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('world', 'NN'),\n",
       " (',', ','),\n",
       " ('for', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('billions', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('billions', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('underprivileged', 'JJ'),\n",
       " ('people', 'NNS'),\n",
       " ('out', 'IN'),\n",
       " ('there', 'EX'),\n",
       " ('who', 'WP'),\n",
       " ('would', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('most', 'RBS'),\n",
       " ('affected', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('.', '.'),\n",
       " ('For', 'IN'),\n",
       " ('our', 'PRP$'),\n",
       " ('children', 'NNS'),\n",
       " ('’', 'VBP'),\n",
       " ('s', 'JJ'),\n",
       " ('children', 'NNS'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('for', 'IN'),\n",
       " ('those', 'DT'),\n",
       " ('people', 'NNS'),\n",
       " ('out', 'RP'),\n",
       " ('there', 'RB'),\n",
       " ('whose', 'WP$'),\n",
       " ('voices', 'NNS'),\n",
       " ('have', 'VBP'),\n",
       " ('been', 'VBN'),\n",
       " ('drowned', 'VBN'),\n",
       " ('out', 'RP'),\n",
       " ('by', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('politics', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('greed', 'NN'),\n",
       " ('.', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('thank', 'VBP'),\n",
       " ('you', 'PRP'),\n",
       " ('all', 'DT'),\n",
       " ('for', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('amazing', 'JJ'),\n",
       " ('award', 'NN'),\n",
       " ('tonight', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Let', 'VB'),\n",
       " ('us', 'PRP'),\n",
       " ('not', 'RB'),\n",
       " ('take', 'VB'),\n",
       " ('this', 'DT'),\n",
       " ('planet', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('granted', 'VBN'),\n",
       " ('.', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('do', 'VBP'),\n",
       " ('not', 'RB'),\n",
       " ('take', 'VB'),\n",
       " ('tonight', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('granted', 'VBN'),\n",
       " ('.', '.'),\n",
       " ('Thank', 'NNP'),\n",
       " ('you', 'PRP'),\n",
       " ('so', 'RB'),\n",
       " ('very', 'RB'),\n",
       " ('much', 'JJ'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tags = []\n",
    "for tw in pot_words_tags:\n",
    "    word_tags.append(tw[0]+\" \"+tw[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
